{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold-Start Evaluation Analysis\n",
    "\n",
    "This notebook provides detailed analysis of model performance on different cold-start scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n",
    "\n",
    "Load results from all cold-start scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('../eval_results/all_splits_davis.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(results).T\n",
    "metrics_df = pd.json_normalize(metrics_df['metrics'])\n",
    "metrics_df.index = results.keys()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Across Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['mse', 'rmse', 'mae', 'r2', 'pearson', 'ci']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    if metric in metrics_df.columns:\n",
    "        ax = axes[idx]\n",
    "        metrics_df[metric].plot(kind='bar', ax=ax)\n",
    "        ax.set_title(f'{metric.upper()}')\n",
    "        ax.set_xlabel('Split Type')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions with uncertainty\n",
    "pred_df = pd.read_csv('../eval_results/davis_random_predictions.csv')\n",
    "\n",
    "# Plot predictions colored by uncertainty\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot with uncertainty\n",
    "scatter = axes[0].scatter(\n",
    "    pred_df['true_affinity'],\n",
    "    pred_df['predicted_affinity'],\n",
    "    c=pred_df['uncertainty'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "axes[0].plot([5, 10], [5, 10], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Affinity')\n",
    "axes[0].set_ylabel('Predicted Affinity')\n",
    "axes[0].set_title('Predictions Colored by Uncertainty')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Uncertainty')\n",
    "\n",
    "# Error vs Uncertainty\n",
    "pred_df['error'] = np.abs(pred_df['true_affinity'] - pred_df['predicted_affinity'])\n",
    "axes[1].scatter(pred_df['uncertainty'], pred_df['error'], alpha=0.6)\n",
    "axes[1].set_xlabel('Uncertainty')\n",
    "axes[1].set_ylabel('Absolute Error')\n",
    "axes[1].set_title('Prediction Error vs Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation between uncertainty and error\n",
    "from scipy.stats import pearsonr\n",
    "corr, pval = pearsonr(pred_df['uncertainty'], pred_df['error'])\n",
    "print(f\"Correlation between uncertainty and error: {corr:.3f} (p={pval:.3e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cold-Start Challenge Analysis\n",
    "\n",
    "Compare performance degradation in different cold-start scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance drop relative to random split\n",
    "baseline = metrics_df.loc['random']\n",
    "performance_drop = pd.DataFrame()\n",
    "\n",
    "for split in ['cold_drug', 'cold_target', 'cold_both']:\n",
    "    if split in metrics_df.index:\n",
    "        drop = (baseline - metrics_df.loc[split]) / baseline * 100\n",
    "        performance_drop[split] = drop\n",
    "\n",
    "print(\"Performance Drop (%) compared to random split:\")\n",
    "print(performance_drop.T)\n",
    "\n",
    "# Visualize\n",
    "performance_drop.T[['rmse', 'r2', 'pearson', 'ci']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Performance Degradation in Cold-Start Scenarios')\n",
    "plt.xlabel('Split Type')\n",
    "plt.ylabel('Performance Drop (%)')\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization\n",
    "\n",
    "Visualize learned attention weights across modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated attention weights\n",
    "modalities = ['Drug', 'Protein', 'TDA']\n",
    "scenarios = ['Random', 'Cold Drug', 'Cold Target', 'Cold Both']\n",
    "\n",
    "# Simulated data\n",
    "attention_weights = np.random.rand(len(scenarios), len(modalities))\n",
    "attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attention_weights,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=modalities,\n",
    "    yticklabels=scenarios\n",
    ")\n",
    "plt.title('Modal Attention Weights Across Scenarios')\n",
    "plt.xlabel('Modality')\n",
    "plt.ylabel('Split Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
